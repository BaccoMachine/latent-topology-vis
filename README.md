
# Neural Manifold Projection: Interactive Analysis of High-Dimensional Embeddings




https://github.com/user-attachments/assets/fe38eac0-8588-48e3-bc63-68d91c543a0e



## Abstract
This project implements an end-to-end framework for the topological analysis of deep neural representations. By training a **Convolutional Autoencoder (CAE)** on the Fashion-MNIST dataset, the system extracts high-level semantic feature vectors (embeddings) from image data. 

The core contribution is an interactive visualization tool that projects the 32-dimensional latent manifold onto a 2D plane using **UMAP (Uniform Manifold Approximation and Projection)**. This allows for real-time inspection of the neural network's internal organization, revealing how the model clusters semantic concepts and manages transitional states between classes.

![Demo Interaction](assets/demo_interaction.gif)

## Project Architecture

The pipeline consists of two distinct stages: **Representation Learning** (Python/TensorFlow) and **Interactive Visualization** (JavaScript/Plotly).

### 1. Representation Learning
The model is a deep Convolutional Autoencoder designed to learn a compressed, dense representation of the input data.
* **Encoder:** A stack of `Conv2D` and `MaxPooling` layers reduces the spatial dimensionality of the input ($28 \times 28$) while increasing feature depth. The final stage flattens the tensor into a **32-dimensional latent vector**, forcing the network to capture the most salient semantic features.
* **Decoder:** Mirrors the encoder using `UpSampling` and Transposed Convolutions to reconstruct the original input, ensuring the latent vector retains structural integrity.
* **Optimization:** Trained using Adam optimizer with Binary Cross-Entropy loss.

### 2. Manifold Projection (UMAP)
To visualize the 32D latent space, we utilize **UMAP**, a manifold learning technique grounded in Riemannian geometry and algebraic topology. Unlike linear methods (e.g., PCA), UMAP constructs a high-dimensional fuzzy topological structure and optimizes a low-dimensional graph to maintain structural isomorphism. This preserves both local neighborhood relations and global structure.



https://github.com/user-attachments/assets/5645b59b-5690-4be2-93ed-514c5b76e2ff




## Interactive Analysis Tool

The web-based interface renders the projected manifold using **WebGL** to handle large-scale scatter plots (10,000+ points) with zero latency.

### Key Features
* **Latent Space Inspection:** Hovering over any data point reveals its ground-truth image alongside its **Latent Activation Map** (the 32D vector visualized as a heatmap).
* **Topology Exploration:** Users can zoom and pan to investigate specific clusters, observing how the network groups similar items (e.g., distinguishing between different types of footwear).
* **Decoupled Architecture:** The visualization logic is decoupled from the inference engine via pre-computed embeddings, allowing the tool to run statically in any browser without backend dependencies.

## Tech Stack

* **Deep Learning:** TensorFlow 2.x / Keras
* **Dimensionality Reduction:** UMAP (Uniform Manifold Approximation and Projection)
* **Data Processing:** NumPy, Scikit-learn
* **Frontend Visualization:** Plotly.js, HTML5 Canvas API, Vanilla JavaScript

## Repository Structure

```text
neural-manifold-projection/
│
├── assets/                  # Documentation images and demos
│   └── demo_interaction.gif
│
├── notebooks/               # Research & Training
│   └── autoencoder_training.ipynb  # Model training, UMAP projection, and Data Export
│
├── web-app/                 # Visualization Tool
│   ├── index.html           # Interactive Dashboard
│   └── data.js              # Pre-computed embeddings & assets (Generated by notebook)
│
├── requirements.txt         # Python dependencies
└── README.md                # Project documentation
````

## How to Reproduce

### 1\. Environment Setup

Install the required Python dependencies:

```bash
pip install -r requirements.txt
```

### 2\. Training & Extraction

Run the Jupyter Notebook located in `notebooks/`. This script will:

1.  Load the dataset (Fashion-MNIST).
2.  Train the Convolutional Autoencoder.
3.  Extract latent vectors and compute UMAP projections.
4.  Export the `data.js` file containing the processed data.

### 3\. Launching the Visualization

Since the tool uses modern web standards, the `data.js` file can be loaded directly.
Simply navigate to the `web-app/` folder and open `index.html` in any modern web browser.

No backend server or GPU inference is required for the visualization phase.

## License

MIT

```
```
